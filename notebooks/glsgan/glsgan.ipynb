{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import importlib\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "from molanet.models.glsgan import GlsGANModel\n",
    "from molanet.models.glsgan import IMAGE_SIZE\n",
    "import molanet.operations as ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(name: str, source_dir, target_dir, size=IMAGE_SIZE):\n",
    "    def transformImageNameSource(name):\n",
    "        return os.path.join(source_dir, name)\n",
    "\n",
    "    def transformImageNameTarget(name: str):\n",
    "        name = name.replace('.jpg', '_Segmentation.png')\n",
    "        return os.path.join(target_dir, name)\n",
    "\n",
    "    source_image = Image.open(transformImageNameSource(name))\n",
    "    target_image = Image.open(transformImageNameTarget(name))\n",
    "\n",
    "    # TODO think about proper resizing... is dis hacky? I don't know\n",
    "    size = size, size\n",
    "    source = source_image.resize(size, Image.BICUBIC)\n",
    "    target = target_image.resize(size, Image.NEAREST)\n",
    "    target = target.convert('1')  # to black and white\n",
    "\n",
    "    return np.array(source).astype(np.float32), np.array(target).astype(np.float32)\n",
    "\n",
    "\n",
    "def get_image_batch(batch_size, source_file_names, source_dir, target_dir) -> [np.ndarray, np.ndarray]:\n",
    "    # TODO chances are we don't get fucked by rng\n",
    "    indices = [random.randint(0, len(source_file_names) - 1) for _ in range(batch_size)]\n",
    "    images = [load_image(source_file_names[i], source_dir, target_dir) for i in indices]\n",
    "    return images\n",
    "\n",
    "def transform_batch(image_batch):\n",
    "    batch_src, batch_target = image_batch[0]\n",
    "    batch_src = (batch_src / 255.0 - 0.5) * 2.0  # Transform into range -1, 1\n",
    "    batch_target = (batch_target - 0.5) * 2.0  # Transform into range -1, 1\n",
    "\n",
    "    batch_src = np.array(batch_src).astype(np.float32)[None, :, :, :]\n",
    "    batch_target = np.array(batch_target).astype(np.float32)[None, :, :, None]\n",
    "    \n",
    "    if(len(image_batch) > 1):\n",
    "        iterimages = iter(image_batch)\n",
    "        next(iterimages) #skip first\n",
    "        for src, target in iterimages:\n",
    "            src = (src / 255.0 - 0.5) * 2.0  # Transform into range -1, 1\n",
    "            target = (target - 0.5) * 2.0  # Transform into range -1, 1\n",
    "            src =np.array(src).astype(np.float32)[None,:, :, :]\n",
    "            target = np.array(target).astype(np.float32)[None,:, :, None]\n",
    "            batch_src = np.concatenate([batch_src,src],axis=0)\n",
    "            batch_target = np.concatenate([batch_target,target],axis=0)\n",
    "    return batch_src, batch_target\n",
    "\n",
    "def save_ndarrays_asimage(filename: str, *arrays: np.ndarray):\n",
    "    def fix_dimensions(array):\n",
    "        if array.ndim > 3 or array.ndim < 2: raise ValueError('arrays must have 2 or 3 dimensions')\n",
    "        if array.ndim == 2:\n",
    "            array = np.repeat(array[:, :, np.newaxis], 3, axis=2)  # go from blackwhite to rgb to make concat work seamless\n",
    "        return array\n",
    "\n",
    "    if len(arrays) > 1:\n",
    "        arrays = [fix_dimensions(array) for array in arrays]\n",
    "        arrays = np.concatenate(arrays, axis=1)\n",
    "\n",
    "    # arrays is just a big 3-dim matrix\n",
    "    im = Image.fromarray(np.uint8(arrays))\n",
    "    im.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(sess, saver, checkpoint_dir, step):\n",
    "    model_name = \"glsgan.model\"\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    saver.save(sess, os.path.join(checkpoint_dir, model_name), global_step=step)\n",
    "\n",
    "\n",
    "def sample_model(filenames: [str], batch_size: int,epoch: int, sess: tf.Session, source_dir: str, target_dir: str, model: GlsGANModel,\n",
    "                 sample_dir: str, max_samples:int=3):\n",
    "    nsamples = min(batch_size, max_samples)\n",
    "\n",
    "    batch = get_image_batch(batch_size, filenames, source_dir, target_dir)    \n",
    "    batch_src, batch_target = transform_batch(batch)\n",
    "   \n",
    "    sample, d_loss, g_loss = sess.run(\n",
    "        [model.fake_B, model.d_loss, model.g_loss],\n",
    "        feed_dict={model.real_data_source: batch_src,\n",
    "                   model.real_data_target: batch_target}\n",
    "    )\n",
    "    \n",
    "    size = sample.shape[2]\n",
    "    zdim = batch_src.shape[-1]\n",
    "    \n",
    "    sample = tf.squeeze(sample).eval()[:nsamples]\n",
    "    batch_src = batch_src[:nsamples]\n",
    "    batch_target = batch_target[:nsamples]\n",
    "    \n",
    "    # from shape [nsamples,size,size,1] : tf.tensor to shape [size*nsamples,size] : ndarray\n",
    "    sample = (np.reshape(sample,[size*nsamples,size]) + 1.0) / 2.0 * 255\n",
    "    original_source = (tf.reshape(batch_src,[size*nsamples,size,zdim]).eval() + 1.0) / 2.0 * 255\n",
    "    original_target = (tf.reshape(tf.squeeze(batch_target),[size*nsamples,size]).eval() + 1.0) / 2.0 * 255\n",
    "    sample_error = np.absolute(original_target - sample)\n",
    "\n",
    "    save_ndarrays_asimage(os.path.join(sample_dir, 'sample_%d.png' % epoch), original_source, sample,\n",
    "                          original_target,sample_error)\n",
    "    print(\"[Sample] d_loss: {:.8f}, g_loss: {:.8f}\".format(d_loss, g_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "source_dir = r'../../../pix2pix-poc-data/training/source'\n",
    "target_dir = r'../../../pix2pix-poc-data/training/target'\n",
    "sourcefiles = [f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f))]\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "sample_dir = \"./samples/sample-%d-%d-%d--%02d%02d\" % (\n",
    "    now.day, now.month, now.year, now.hour, now.minute)  # Generated samples\n",
    "checkpoint_dir = \"./checkpoints\"  # Model\n",
    "if not os.path.exists('./samples'):\n",
    "    os.mkdir('./samples')\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.mkdir(sample_dir)\n",
    "\n",
    "batch_size = 5\n",
    "size = IMAGE_SIZE\n",
    "num_feature_maps = 64\n",
    "is_grayscale = False\n",
    "L1_lambda_lsgan = 100\n",
    "L1_lambda_generator = 100\n",
    "glsgan_alpha = 0 # ls-gan\n",
    "iterations = 50000\n",
    "N = 1\n",
    "\n",
    "restore_iteration = None\n",
    "use_random_image_as_sample = True\n",
    "max_samples = 5\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = GlsGANModel(batch_size=batch_size, \n",
    "                        image_size=size, \n",
    "                        src_color_channels=3, \n",
    "                        target_color_channels=1,\n",
    "                        g_l1_lambda=L1_lambda_generator)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    if restore_iteration is not None and restore_iteration > 0:\n",
    "        iration_start = restore_iteration + 1\n",
    "        checkpoint = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        checkpoint_name = os.path.basename(checkpoint.model_checkpoint_path)\n",
    "        print('checkpoint_name=' + str(checkpoint_name))\n",
    "        saver.restore(sess, os.path.join(checkpoint_dir, checkpoint_name))\n",
    "    else:\n",
    "        iteration_start = 0\n",
    "        \n",
    "    #make glsgan\n",
    "    #see glsgan paper and https://github.com/guojunq/glsgan/blob/master/glsgan.lua#L257\n",
    "    def l1diff(x,y):\n",
    "        dist = tf.reduce_sum(tf.abs(tf.round(y)-tf.round(x)))\n",
    "        return dist\n",
    "    pdist = L1_lambda_lsgan * l1diff(model.real_B,model.fake_B)\n",
    "    cost1 = (pdist + model.d_loss_real - model.d_loss_fake)\n",
    "\n",
    "    glsloss =ops.leaky_relu(cost1,glsgan_alpha)\n",
    "    #self.d_error_hinge = tf.reduce_mean(self.glsloss)\n",
    "\n",
    "    # Optimizers\n",
    "    disc_optim = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)  # TODO: pix2pix params\n",
    "    gen_optim = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)  # TODO: pix2pix params\n",
    "\n",
    "    disc_update_step = disc_optim.minimize(glsloss, var_list=model.d_vars)\n",
    "    gen_update_step = gen_optim.minimize(model.g_loss, var_list=model.g_vars)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # logging\n",
    "    writer = tf.summary.FileWriter(\"./logs\", sess.graph)\n",
    "    g_sum = tf.summary.merge([model.d__sum,\n",
    "                              model.fake_B_sum, model.d_loss_fake_sum, model.g_loss_sum])\n",
    "    d_sum = tf.summary.merge([model.d_sum, model.d_loss_real_sum, model.d_loss_sum])\n",
    "\n",
    "    start_time = time.time()\n",
    "    for iteration in range(iteration_start, iterations):\n",
    "        batch = get_image_batch(batch_size, sourcefiles,\n",
    "                                source_dir=source_dir,\n",
    "                                target_dir=target_dir)\n",
    "        batch_src,batch_target = transform_batch(batch)\n",
    "\n",
    "        #as proposed in glsgan paper update discriminator N time every iteration\n",
    "        for i in range(0,N):\n",
    "            _, summary_str = sess.run([disc_update_step, d_sum],\n",
    "                                      feed_dict={model.real_data_source: batch_src,\n",
    "                                                 model.real_data_target: batch_target})\n",
    "\n",
    "            writer.add_summary(summary_str, iteration)\n",
    "\n",
    "        # Update G network\n",
    "        _, summary_str = sess.run([gen_update_step, g_sum],\n",
    "                                  feed_dict={model.real_data_source: batch_src,\n",
    "                                             model.real_data_target: batch_target})\n",
    "        writer.add_summary(summary_str, iteration)\n",
    "        #_, summary_str = sess.run([gen_update_step, g_sum],\n",
    "                                  #feed_dict={model.real_data_source: batch_src,\n",
    "                                            # model.real_data_target: batch_target})\n",
    "        #writer.add_summary(summary_str, iteration)\n",
    "\n",
    "        errD_fake = model.d_loss_fake.eval(\n",
    "            {model.real_data_target: batch_target, model.real_data_source: batch_src})\n",
    "        errD_real = model.d_loss_real.eval(\n",
    "            {model.real_data_target: batch_target, model.real_data_source: batch_src})\n",
    "        errG = model.g_loss.eval({model.real_data_target: batch_target, model.real_data_source: batch_src})\n",
    "\n",
    "        print(\"Epoch: [%2d] time: %4.4f, d_loss: %.8f, g_loss: %.8f\" \\\n",
    "              % (iteration, time.time() - start_time, errD_fake + errD_real, errG))\n",
    "\n",
    "        if iteration % 50 == 1:\n",
    "            # gib nice picture output :)\n",
    "            sample_model(sourcefiles, batch_size, iteration, sess, source_dir, target_dir, model, sample_dir,\n",
    "                         max_samples=max_samples)\n",
    "\n",
    "        if iteration % 500 == 2:\n",
    "            save(sess, saver, checkpoint_dir, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".python",
   "language": "python",
   "name": ".python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
