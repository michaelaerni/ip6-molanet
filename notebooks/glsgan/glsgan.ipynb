{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import importlib\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "\n",
    "from molanet.models.glsgan import GlsGANModel\n",
    "from molanet.models.glsgan import IMAGE_SIZE\n",
    "import molanet.operations as ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(name: str, source_dir, target_dir, size=IMAGE_SIZE):\n",
    "    def transformImageNameSource(name):\n",
    "        return os.path.join(source_dir, name)\n",
    "\n",
    "    def transformImageNameTarget(name: str):\n",
    "        name = name.replace('.jpg', '_Segmentation.png')\n",
    "        return os.path.join(target_dir, name)\n",
    "\n",
    "    source_image = Image.open(transformImageNameSource(name))\n",
    "    target_image = Image.open(transformImageNameTarget(name))\n",
    "\n",
    "    # TODO think about proper resizing... is dis hacky? I don't know\n",
    "    size = size, size\n",
    "    source = source_image.resize(size, Image.BICUBIC)\n",
    "    target = target_image.resize(size, Image.NEAREST)\n",
    "    target = target.convert('1')  # to black and white\n",
    "\n",
    "    return np.array(source).astype(np.float32), np.array(target).astype(np.float32)\n",
    "\n",
    "\n",
    "def get_image_batch(batch_size, source_file_names, source_dir, target_dir) -> [np.ndarray, np.ndarray]:\n",
    "    # TODO chances are we don't get fucked by rng\n",
    "    indices = [random.randint(0, len(source_file_names) - 1) for _ in range(batch_size)]\n",
    "    images = [load_image(source_file_names[i], source_dir, target_dir) for i in indices]\n",
    "    return images\n",
    "\n",
    "def transform_batch(image_batch):\n",
    "    batch_src, batch_target = image_batch[0]\n",
    "    batch_src = (batch_src / 255.0 - 0.5) * 2.0  # Transform into range -1, 1\n",
    "    batch_target = (batch_target - 0.5) * 2.0  # Transform into range -1, 1\n",
    "\n",
    "    batch_src = np.array(batch_src).astype(np.float32)[None, :, :, :]\n",
    "    batch_target = np.array(batch_target).astype(np.float32)[None, :, :, None]\n",
    "    \n",
    "    if(len(image_batch) > 1):\n",
    "        iterimages = iter(image_batch)\n",
    "        next(iterimages) #skip first\n",
    "        for src, target in iterimages:\n",
    "            src = (src / 255.0 - 0.5) * 2.0  # Transform into range -1, 1\n",
    "            target = (target - 0.5) * 2.0  # Transform into range -1, 1\n",
    "            src =np.array(src).astype(np.float32)[None,:, :, :]\n",
    "            target = np.array(target).astype(np.float32)[None,:, :, None]\n",
    "            batch_src = np.concatenate([batch_src,src],axis=0)\n",
    "            batch_target = np.concatenate([batch_target,target],axis=0)\n",
    "    return batch_src, batch_target\n",
    "\n",
    "def save_ndarrays_asimage(filename: str, *arrays: np.ndarray):\n",
    "    def fix_dimensions(array):\n",
    "        if array.ndim > 3 or array.ndim < 2: raise ValueError('arrays must have 2 or 3 dimensions')\n",
    "        if array.ndim == 2:\n",
    "            array = np.repeat(array[:, :, np.newaxis], 3, axis=2)  # go from blackwhite to rgb to make concat work seamless\n",
    "        return array\n",
    "\n",
    "    if len(arrays) > 1:\n",
    "        arrays = [fix_dimensions(array) for array in arrays]\n",
    "        arrays = np.concatenate(arrays, axis=1)\n",
    "\n",
    "    # arrays is just a big 3-dim matrix\n",
    "    im = Image.fromarray(np.uint8(arrays))\n",
    "    im.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(sess, saver, checkpoint_dir, step):\n",
    "    model_name = \"glsgan.model\"\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    saver.save(sess, os.path.join(checkpoint_dir, model_name), global_step=step)\n",
    "\n",
    "\n",
    "def sample_model(filenames: [str], batch_size: int,epoch: int, sess: tf.Session, source_dir: str, target_dir: str, model: GlsGANModel,\n",
    "                 sample_dir: str, max_samples:int=3):\n",
    "    nsamples = min(batch_size, max_samples)\n",
    "\n",
    "    batch = get_image_batch(batch_size, filenames, source_dir, target_dir)    \n",
    "    batch_src, batch_target = transform_batch(batch)\n",
    "   \n",
    "    sample, d_loss, g_loss = sess.run(\n",
    "        [model.fake_B, model.d_loss, model.g_loss],\n",
    "        feed_dict={model.real_data_source: batch_src,\n",
    "                   model.real_data_target: batch_target}\n",
    "    )\n",
    "    \n",
    "    size = sample.shape[2]\n",
    "    zdim = batch_src.shape[-1]\n",
    "    \n",
    "    sample = tf.squeeze(sample).eval()[:nsamples]\n",
    "    batch_src = batch_src[:nsamples]\n",
    "    batch_target = batch_target[:nsamples]\n",
    "    \n",
    "    # from shape [nsamples,size,size,1] : tf.tensor to shape [size*nsamples,size] : ndarray\n",
    "    sample = (np.reshape(sample,[size*nsamples,size]) + 1.0) / 2.0 * 255\n",
    "    original_source = (tf.reshape(batch_src,[size*nsamples,size,zdim]).eval() + 1.0) / 2.0 * 255\n",
    "    original_target = (tf.reshape(tf.squeeze(batch_target),[size*nsamples,size]).eval() + 1.0) / 2.0 * 255\n",
    "\n",
    "    save_ndarrays_asimage(os.path.join(sample_dir, 'sample_%d.png' % epoch), original_source, sample,\n",
    "                          original_target,sample_error)\n",
    "    print(\"[Sample] d_loss: {:.8f}, g_loss: {:.8f}\".format(d_loss, g_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0] time: 3.6876, d_loss: 1.37057042, g_loss: 91.46552277\n",
      "Epoch: [ 1] time: 7.0418, d_loss: 1.41155851, g_loss: 85.44960022\n",
      "[Sample] d_loss: 1.45718765, g_loss: 88.19380951\n",
      "Epoch: [ 2] time: 11.3903, d_loss: 1.79811370, g_loss: 78.61933899\n",
      "Epoch: [ 3] time: 19.8668, d_loss: 2.67224097, g_loss: 75.88973236\n",
      "Epoch: [ 4] time: 23.9828, d_loss: 4.53027916, g_loss: 73.36322021\n",
      "Epoch: [ 5] time: 27.8095, d_loss: 11.80871677, g_loss: 67.94054413\n",
      "Epoch: [ 6] time: 30.7866, d_loss: 20.55378151, g_loss: 72.24281311\n",
      "Epoch: [ 7] time: 34.2821, d_loss: 36.88986969, g_loss: 55.82383728\n",
      "Epoch: [ 8] time: 37.6691, d_loss: 79.97319031, g_loss: 77.56661987\n",
      "Epoch: [ 9] time: 41.0316, d_loss: 131.47407532, g_loss: 51.30364227\n",
      "Epoch: [10] time: 44.1608, d_loss: 248.14453125, g_loss: 45.16580582\n",
      "Epoch: [11] time: 47.5756, d_loss: 379.81887817, g_loss: 48.67750549\n",
      "Epoch: [12] time: 51.2167, d_loss: 845.70031738, g_loss: 49.71894455\n",
      "Epoch: [13] time: 54.7719, d_loss: 953.14160156, g_loss: 43.22514343\n",
      "Epoch: [14] time: 57.9364, d_loss: 1229.45776367, g_loss: 66.84191895\n",
      "Epoch: [15] time: 61.6670, d_loss: 1561.25610352, g_loss: 38.99604034\n",
      "Epoch: [16] time: 65.2020, d_loss: 2751.36206055, g_loss: 35.82040787\n",
      "Epoch: [17] time: 69.0423, d_loss: 2148.69775391, g_loss: 48.52866364\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "source_dir = r'../../../pix2pix-poc-data/training/source'\n",
    "target_dir = r'../../../pix2pix-poc-data/training/target'\n",
    "sourcefiles = [f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f))]\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "sample_dir = \"./samples/sample-%d-%d-%d--%02d%02d\" % (\n",
    "    now.day, now.month, now.year, now.hour, now.minute)  # Generated samples\n",
    "checkpoint_dir = \"./checkpoints\"  # Model\n",
    "if not os.path.exists('./samples'):\n",
    "    os.mkdir('./samples')\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.mkdir(sample_dir)\n",
    "\n",
    "batch_size = 5\n",
    "size = IMAGE_SIZE\n",
    "num_feature_maps = 64\n",
    "is_grayscale = False\n",
    "L1_lambda = 100\n",
    "glsgan_alpha = 0 # ls-gan\n",
    "iterations = 50000\n",
    "N = 1\n",
    "\n",
    "restore_iteration = None\n",
    "use_random_image_as_sample = True\n",
    "max_samples = 5\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = GlsGANModel(batch_size=batch_size, image_size=size, src_color_channels=3, target_color_channels=1,l1_lambda=L1_lambda)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    if restore_iteration is not None and restore_iteration > 0:\n",
    "        iration_start = restore_iteration + 1\n",
    "        checkpoint = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        checkpoint_name = os.path.basename(checkpoint.model_checkpoint_path)\n",
    "        print('checkpoint_name=' + str(checkpoint_name))\n",
    "        saver.restore(sess, os.path.join(checkpoint_dir, checkpoint_name))\n",
    "    else:\n",
    "        iteration_start = 0\n",
    "        \n",
    "    #make glsgan\n",
    "    #see glsgan paper and https://github.com/guojunq/glsgan/blob/master/glsgan.lua#L257\n",
    "    def l1diff(x,y):\n",
    "        dist = tf.reduce_sum(tf.abs(tf.round(y)-tf.round(x)))\n",
    "        return dist\n",
    "    pdist = L1_lambda * l1diff(model.real_B,model.fake_B)\n",
    "    cost1 = pdist + model.d_loss_real - model.d_loss_fake\n",
    "\n",
    "    glsloss =ops.leaky_relu(cost1,glsgan_alpha)\n",
    "    #self.d_error_hinge = tf.reduce_mean(self.glsloss)\n",
    "\n",
    "    # Optimizers\n",
    "    disc_optim = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)  # TODO: pix2pix params\n",
    "    gen_optim = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)  # TODO: pix2pix params\n",
    "\n",
    "    disc_update_step = disc_optim.minimize(glsloss, var_list=model.d_vars)\n",
    "    gen_update_step = gen_optim.minimize(model.g_loss, var_list=model.g_vars)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # logging\n",
    "    writer = tf.summary.FileWriter(\"./logs\", sess.graph)\n",
    "    g_sum = tf.summary.merge([model.d__sum,\n",
    "                              model.fake_B_sum, model.d_loss_fake_sum, model.g_loss_sum])\n",
    "    d_sum = tf.summary.merge([model.d_sum, model.d_loss_real_sum, model.d_loss_sum])\n",
    "\n",
    "    start_time = time.time()\n",
    "    for iteration in range(iteration_start, iterations):\n",
    "        batch = get_image_batch(batch_size, sourcefiles,\n",
    "                                source_dir=source_dir,\n",
    "                                target_dir=target_dir)\n",
    "        batch_src,batch_target = transform_batch(batch)\n",
    "\n",
    "        #as proposed in glsgan paper update discriminator N time every iteration\n",
    "        for i in range(0,N):\n",
    "            _, summary_str = sess.run([disc_update_step, d_sum],\n",
    "                                      feed_dict={model.real_data_source: batch_src,\n",
    "                                                 model.real_data_target: batch_target})\n",
    "\n",
    "            writer.add_summary(summary_str, iteration)\n",
    "\n",
    "        # Update G network\n",
    "        _, summary_str = sess.run([gen_update_step, g_sum],\n",
    "                                  feed_dict={model.real_data_source: batch_src,\n",
    "                                             model.real_data_target: batch_target})\n",
    "        writer.add_summary(summary_str, iteration)\n",
    "        #_, summary_str = sess.run([gen_update_step, g_sum],\n",
    "                                  #feed_dict={model.real_data_source: batch_src,\n",
    "                                            # model.real_data_target: batch_target})\n",
    "        #writer.add_summary(summary_str, iteration)\n",
    "\n",
    "        errD_fake = model.d_loss_fake.eval(\n",
    "            {model.real_data_target: batch_target, model.real_data_source: batch_src})\n",
    "        errD_real = model.d_loss_real.eval(\n",
    "            {model.real_data_target: batch_target, model.real_data_source: batch_src})\n",
    "        errG = model.g_loss.eval({model.real_data_target: batch_target, model.real_data_source: batch_src})\n",
    "\n",
    "        print(\"Epoch: [%2d] time: %4.4f, d_loss: %.8f, g_loss: %.8f\" \\\n",
    "              % (iteration, time.time() - start_time, errD_fake + errD_real, errG))\n",
    "\n",
    "        if iteration % 50 == 1:\n",
    "            # gib nice picture output :)\n",
    "            sample_model(sourcefiles, batch_size, iteration, sess, source_dir, target_dir, model, sample_dir,\n",
    "                         max_samples=max_samples)\n",
    "\n",
    "        if iteration % 500 == 2:\n",
    "            save(sess, saver, checkpoint_dir, iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".python",
   "language": "python",
   "name": ".python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
