{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../../\")) # Add path to root for imports\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import png # Image saving\n",
    "import math\n",
    "\n",
    "# Load MNIST data for experiments\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_size = 100\n",
    "batch_size = 128\n",
    "class_count = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import molanet.models.dcgan as model\n",
    "import importlib\n",
    "importlib.reload(model) # Reload for fast prototyping\n",
    "\n",
    "# Reset variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "z = tf.placeholder(tf.float32, [None, z_size + class_count + 1])\n",
    "x = tf.placeholder(tf.float32, [None, 32, 32, 1])\n",
    "y = tf.placeholder(tf.float32, [None, class_count + 1])\n",
    "\n",
    "# Models\n",
    "gen_net, w_gen, b_gen = model.dcgan_generator(z, batch_size=batch_size)\n",
    "disc_real_net, disc_real_logits, w_disc, b_disc = model.dcgan_discriminator(x, class_count + 1)\n",
    "disc_gen_net, disc_gen_logits, _, _ = model.dcgan_discriminator(gen_net, class_count + 1, reuse=True)\n",
    "\n",
    "# Loss functions\n",
    "fake_logits = np.zeros((batch_size, class_count + 1)).astype(np.float32)\n",
    "fake_logits[:, -1] = 1 # Last class = fake\n",
    "disc_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_real_logits, labels=y))\n",
    "disc_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_gen_logits, labels=fake_logits))\n",
    "disc_loss = disc_loss_real + disc_loss_fake\n",
    "gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_gen_logits, labels=y))\n",
    "\n",
    "trainable_variables = tf.trainable_variables()\n",
    "disc_vars = [var for var in trainable_variables if var.name.startswith(\"discriminator\")]\n",
    "gen_vars = [var for var in trainable_variables if var.name.startswith(\"generator\")]\n",
    "\n",
    "# Optimizers\n",
    "disc_optim = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)\n",
    "gen_optim = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)\n",
    "\n",
    "disc_update_step = disc_optim.minimize(disc_loss, var_list=disc_vars)\n",
    "gen_update_step = gen_optim.minimize(gen_loss, var_list=gen_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterations = 500000\n",
    "sample_directory = \"./samples\" # Generated samples\n",
    "model_directory = \"./models\" # Model\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        # Random z batch\n",
    "        zs = np.random.uniform(-1.0, 1.0, size=[batch_size, z_size]).astype(np.float32)\n",
    "        zs_class = np.zeros((batch_size, class_count + 1))\n",
    "        for idx in range(batch_size):\n",
    "            zs_class[idx, np.random.randint(0, class_count)] = 1.0\n",
    "        zs = np.concatenate([zs, zs_class], axis=1)\n",
    "\n",
    "        # \"Real\" input images\n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        ys = np.zeros((batch_size, class_count + 1)).astype(np.float32)\n",
    "        # TODO: More intelligent one-hot\n",
    "        for idx, train_y in enumerate(batch[1]):\n",
    "            ys[idx, :] = np.concatenate([train_y, [0.0]])\n",
    "\n",
    "        xs = batch[0]\n",
    "        xs = (np.reshape(xs, [batch_size, 28, 28, 1]) - 0.5) * 2.0 # Transform into range -1, 1\n",
    "        xs = np.pad(xs, [[0, 0], [2, 2], [2, 2], [0, 0]], \"constant\", constant_values=[-1, -1]) # Pad to have correct size (28x28 -> 32x32)\n",
    "\n",
    "        # Update discriminator\n",
    "        _, d_loss = sess.run([disc_update_step, disc_loss], feed_dict={z: zs, x: xs, y: ys})\n",
    "\n",
    "        # Update generator (twice)\n",
    "        _, g_loss = sess.run([gen_update_step, gen_loss], feed_dict={z: zs, y: zs_class})\n",
    "        _, g_loss = sess.run([gen_update_step, gen_loss], feed_dict={z: zs, y: zs_class})\n",
    "\n",
    "        # Print loss\n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Loss: generator={g_loss},\\tdiscriminator={d_loss}\")\n",
    "            \n",
    "            # Generate sample images\n",
    "            z_sample = np.random.uniform(-1.0, 1.0, size=[batch_size, z_size]).astype(np.float32)\n",
    "            z_sample_class = np.zeros((batch_size, class_count + 1))\n",
    "            z_sample_class[0:class_count, :-1] = np.eye(class_count)\n",
    "\n",
    "            z_sample = np.concatenate([z_sample, z_sample_class], axis=1)\n",
    "\n",
    "            sample_images = sess.run(gen_net, feed_dict={z: z_sample})\n",
    "\n",
    "            # Save sample images\n",
    "            if not os.path.exists(sample_directory):\n",
    "                os.makedirs(sample_directory)\n",
    "\n",
    "            raw_images = np.reshape(sample_images[0:class_count], [class_count, 32, 32])\n",
    "\n",
    "            # TODO: tanh is never exactly -1 or 1!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "            raw_images = (raw_images + 1.0) / 2.0 * 255.0 # Transform to range 0, 255\n",
    "            save_path = f\"{sample_directory}/sample_{iteration}.png\"\n",
    "\n",
    "            # Generate figure\n",
    "            height = raw_images.shape[1]\n",
    "            width = raw_images.shape[2]\n",
    "            count_y = int(math.sqrt(class_count))\n",
    "            count_x = int(math.ceil(math.sqrt(class_count)))\n",
    "            sample_figure = np.zeros((count_y * height, count_x * width))\n",
    "            for idx, image in enumerate(raw_images):\n",
    "                i = idx % count_x\n",
    "                j = idx // count_x\n",
    "                sample_figure[j * height:(j + 1) * height, i * width:(i + 1) * width] = image\n",
    "            \n",
    "            with open(save_path, 'w+b') as f:\n",
    "                png.Writer(sample_figure.shape[1], sample_figure.shape[0], greyscale=True).write(f, sample_figure)\n",
    "\n",
    "        # Save model\n",
    "        if iteration % 1000 == 0:\n",
    "            if not os.path.exists(model_directory):\n",
    "                os.makedirs(model_directory)\n",
    "        \n",
    "            saver.save(sess, f\"{model_directory}/model-{iteration}.cptk\")\n",
    "            print(f\"Saved model from iteration {iteration}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
